{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TRAIN.ipynb","provenance":[],"authorship_tag":"ABX9TyMvwRzPIJ77FKNOES5nk40g"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"vZxZOwPGX7U5","executionInfo":{"status":"ok","timestamp":1601639377696,"user_tz":-330,"elapsed":5607,"user":{"displayName":"smriti gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi61FdzStVeI4CbVBMrhSuqWIHqEw_fypYilNr4=s64","userId":"15960455078015597961"}}},"source":["import numpy as np\n","import pandas as pd\n","import scipy.io\n","import imageio\n","import h5py\n","import os\n","import matplotlib\n","import matplotlib.colors\n","import matplotlib.pyplot as plt\n","import skimage.transform \n","import random\n","import torchvision\n","import torch\n","from PIL import Image\n","from skimage.io import imread_collection\n","import imageio\n","from skimage import transform\n","import matplotlib.image as mpimg\n","from PIL import Image\n","import glob\n","import argparse\n","import os\n","import time\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","import torch.optim\n","from torch.optim.lr_scheduler import LambdaLR\n","import torchvision.transforms as transforms\n","from torchvision.utils import make_grid\n","from torch import nn\n","import torch.nn.functional as F\n","import easydict\n","from sklearn.metrics import mean_squared_error \n","from torch.autograd.variable import Variable\n","from skimage.color import rgb2gray\n","#from tensorboardX import SummaryWriter\n","# from ipynb.fs.full.utils import load_ckpt\n","# from ipynb.fs.full.utils import save_ckpt\n","# from ipynb.fs.full.utils import print_log\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"craubecPYmJx","executionInfo":{"status":"ok","timestamp":1601639377699,"user_tz":-330,"elapsed":5128,"user":{"displayName":"smriti gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi61FdzStVeI4CbVBMrhSuqWIHqEw_fypYilNr4=s64","userId":"15960455078015597961"}},"outputId":"ea33eb88-6da5-44f5-8dff-5cc7c594362f","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"njAB60kwYAda","executionInfo":{"status":"ok","timestamp":1601639384607,"user_tz":-330,"elapsed":11655,"user":{"displayName":"smriti gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi61FdzStVeI4CbVBMrhSuqWIHqEw_fypYilNr4=s64","userId":"15960455078015597961"}}},"source":["visible_file_location = '/content/drive/My Drive/dataset/vis/*.bmp'\n","infrared_file_location = '/content/drive/My Drive/dataset/ir/*.bmp'\n","\n","##DON'T ALTER AFTER THIS\n","\n","#have to change in accordance to azure\n","image_list_1 = []\n","name_1=[]\n","for filename in glob.glob(infrared_file_location): #assuming gif\n","    im=Image.open(filename)\n","    name_1.append(filename.split('/')[-1].split('_')[0])    \n","    image_list_1.append(im)\n","image_list_2 = []\n","name_2=[]\n","for filename in glob.glob(visible_file_location): #assuming gif   \n","    im=Image.open(filename)\n","    image_list_2.append(im)    \n","    name_2.append(filename.split('/')[-1].split('_')[0])"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"uMAGYmkxZsnG","executionInfo":{"status":"ok","timestamp":1601639401681,"user_tz":-330,"elapsed":28338,"user":{"displayName":"smriti gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi61FdzStVeI4CbVBMrhSuqWIHqEw_fypYilNr4=s64","userId":"15960455078015597961"}}},"source":["a=pd.DataFrame(name_1).reset_index()\n","a[\"image\"]=image_list_1\n","\n","b=pd.DataFrame(name_2).reset_index()\n","b[\"image\"]=image_list_2\n","\n","a=a.merge(b,on=[0],how=\"inner\")\n","\n","vis_list=list(a[\"index_x\"])\n","ir_list=list(a[\"index_y\"])\n","\n","Z_ir1=list(a[\"image_x\"])\n","Z_vis1=list(a[\"image_y\"])\n","\n","Z_ir = [np.asarray(x.resize([640,480])) for x in Z_ir1]\n","Z_vis = [rgb2gray(np.asarray(x.resize([640,480]))) for x in Z_vis1]"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y2zmwCIKYhpV","executionInfo":{"status":"ok","timestamp":1601639401687,"user_tz":-330,"elapsed":27994,"user":{"displayName":"smriti gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi61FdzStVeI4CbVBMrhSuqWIHqEw_fypYilNr4=s64","userId":"15960455078015597961"}}},"source":["!cd '/content/drive/My Drive/dataset'"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7LrLKBFMZAvS"},"source":["## **DATA LOADER** => NO CHANGE"]},{"cell_type":"code","metadata":{"id":"2nM6pYziY8Ln","executionInfo":{"status":"ok","timestamp":1601639401690,"user_tz":-330,"elapsed":27092,"user":{"displayName":"smriti gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi61FdzStVeI4CbVBMrhSuqWIHqEw_fypYilNr4=s64","userId":"15960455078015597961"}}},"source":["\n","#from RedNet_train import image_h, image_w\n","\n","class SUNRGBD(Dataset):\n","    def __init__(self, transform=None, phase_train=True):\n","\n","        self.phase_train = phase_train\n","        self.transform = transform\n","        self.img_dir_train=Z_vis\n","        self.depth_dir_train=Z_ir\n","        \n","\n","    def __len__(self):\n","        if self.phase_train:\n","            return len(self.img_dir_train)\n","        else:\n","            return len(self.img_dir_test)\n","\n","    def __getitem__(self, idx):\n","        if self.phase_train:\n","            img_dir = self.img_dir_train\n","            depth_dir = self.depth_dir_train\n","        image = img_dir[idx]\n","        depth = depth_dir[idx]\n","\n","        sample = {'image': image, 'depth': depth}\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","            return sample\n","\n","\n","class RandomHSV(object):\n","    \"\"\"\n","        Args:\n","            h_range (float tuple): random ratio of the hue channel,\n","                new_h range from h_range[0]*old_h to h_range[1]*old_h.\n","            s_range (float tuple): random ratio of the saturation channel,\n","                new_s range from s_range[0]*old_s to s_range[1]*old_s.\n","            v_range (int tuple): random bias of the value channel,\n","                new_v range from old_v-v_range to old_v+v_range.\n","        Notice:\n","            h range: 0-1\n","            s range: 0-1\n","            v range: 0-255\n","        \"\"\"\n","\n","    def __init__(self, h_range, s_range, v_range):\n","        assert isinstance(h_range, (list, tuple)) and \\\n","               isinstance(s_range, (list, tuple)) and \\\n","               isinstance(v_range, (list, tuple))\n","        self.h_range = h_range\n","        self.s_range = s_range\n","        self.v_range = v_range\n","\n","    def __call__(self, sample):\n","        img = sample['image']\n","        img_hsv = matplotlib.colors.rgb_to_hsv(img)\n","        img_h, img_s, img_v = img_hsv[:, :, 0], img_hsv[:, :, 1], img_hsv[:, :, 2]\n","        h_random = np.random.uniform(min(self.h_range), max(self.h_range))\n","        s_random = np.random.uniform(min(self.s_range), max(self.s_range))\n","        v_random = np.random.uniform(-min(self.v_range), max(self.v_range))\n","        img_h = np.clip(img_h * h_random, 0, 1)\n","        img_s = np.clip(img_s * s_random, 0, 1)\n","        img_v = np.clip(img_v + v_random, 0, 255)\n","        img_hsv = np.stack([img_h, img_s, img_v], axis=2)\n","        img_new = matplotlib.colors.hsv_to_rgb(img_hsv)\n","        return {'image': img_new, 'depth': sample['depth']}\n","\n","\n","class scaleNorm(object):\n","    def __call__(self, sample):\n","        image, depth = sample['image'], sample['depth']\n","        \n","\n","        # Bi-linear\n","        image = skimage.transform.resize(image, (image_h, image_w), order=1,\n","                                         mode='reflect', preserve_range=True)\n","        # Nearest-neighbor\n","        depth = skimage.transform.resize(depth, (image_h, image_w), order=0,\n","                                         mode='reflect', preserve_range=True)\n","#         print(\"scalenorm\")\n","        return {'image': image, 'depth': depth}\n","\n","\n","class RandomScale(object):\n","    def __init__(self, scale):\n","        self.scale_low = min(scale)\n","        self.scale_high = max(scale)\n","\n","    def __call__(self, sample):\n","        image, depth = sample['image'], sample['depth']\n","        target_scale = random.uniform(self.scale_low, self.scale_high)\n","        # (H, W, C)\n","        target_height = int(round(target_scale * image.shape[0]))\n","        target_width = int(round(target_scale * image.shape[1]))\n","        # Bi-linear\n","        image = skimage.transform.resize(image, (target_height, target_width),\n","                                         order=1, mode='reflect', preserve_range=True)\n","        # Nearest-neighbor\n","        depth = skimage.transform.resize(depth, (target_height, target_width),\n","                                         order=0, mode='reflect', preserve_range=True)\n","        \n","        \n","        return {'image': image, 'depth': depth}\n","\n","\n","class RandomCrop(object):\n","    def __init__(self, th, tw):\n","        self.th = int(th)\n","        self.tw = int(tw)\n","        print('th')\n","        print(th)\n","\n","    def __call__(self, sample):\n","        image, depth = sample['image'], sample['depth']\n","        h = image.shape[0]\n","        w = image.shape[1]\n","        print('h')\n","        print(h)\n","        print('diff')\n","        print(self.th)\n","        a=int(self.th)\n","        b=int( self.tw)\n","        print(a)\n","        print(b)\n","        i = random.randint(0,a)\n","        j = random.randint(0,b)\n","        return {'image': image[i:i + image_h, j:j + image_w, :],\n","                'depth': depth[i:i + image_h, j:j + image_w]}\n","  \n","class RandomFlip(object):\n","    def __call__(self, sample):\n","        image, depth = sample['image'], sample['depth']\n","        if random.random() > 0.5:\n","            image = np.fliplr(image).copy()\n","            depth = np.fliplr(depth).copy()\n","        return {'image': image, 'depth': depth}\n","\n","\n","# Transforms on torch.*Tensor\n","class Normalize(object):\n","    def __call__(self, sample):\n","        image, depth = sample['image'], sample['depth']\n","        image = image / 255\n","        image = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                                 std=[0.229, 0.224, 0.225])(image)\n","        depth = torchvision.transforms.Normalize(mean=[19050],\n","                                                std=[9650])(depth)\n","        sample['image'] = image\n","        sample['depth'] = depth\n","        return sample\n","\n","\n","class ToTensor(object):\n","    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n","\n","    def __call__(self, sample):\n","        image, depth= sample['image'], sample['depth']\n","        image = np.expand_dims(image, 0).astype(np.float)\n","\n","        depth = np.expand_dims(depth, 0).astype(np.float)\n","        depth = depth/255\n","        return {'image': torch.from_numpy(image).float(),\n","              'depth': torch.from_numpy(depth).float()}"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nZRWoyxDZGfy"},"source":["## **UTILS** => NO CHANGE"]},{"cell_type":"code","metadata":{"id":"suGZ_ZrcZFjG","executionInfo":{"status":"ok","timestamp":1601639402147,"user_tz":-330,"elapsed":26665,"user":{"displayName":"smriti gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi61FdzStVeI4CbVBMrhSuqWIHqEw_fypYilNr4=s64","userId":"15960455078015597961"}}},"source":["med_frq = [0.382900, 0.452448, 0.637584, 0.377464, 0.585595,\n","           0.479574, 0.781544, 0.982534, 1.017466, 0.624581,\n","           2.589096, 0.980794, 0.920340, 0.667984, 1.172291,\n","           0.862240, 0.921714, 2.154782, 1.187832, 1.178115,\n","           1.848545, 1.428922, 2.849658, 0.771605, 1.656668,\n","           4.483506, 2.209922, 1.120280, 2.790182, 0.706519,\n","           3.994768, 2.220004, 0.972934, 1.481525, 5.342475,\n","           0.750738, 4.040773]\n","\n","model_urls = {\n","    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n","    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n","    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n","    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n","    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n","}\n","\n","label_colours = [(0, 0, 0),\n","                 # 0=background\n","                 (148, 65, 137), (255, 116, 69), (86, 156, 137),\n","                 (202, 179, 158), (155, 99, 235), (161, 107, 108),\n","                 (133, 160, 103), (76, 152, 126), (84, 62, 35),\n","                 (44, 80, 130), (31, 184, 157), (101, 144, 77),\n","                 (23, 197, 62), (141, 168, 145), (142, 151, 136),\n","                 (115, 201, 77), (100, 216, 255), (57, 156, 36),\n","                 (88, 108, 129), (105, 129, 112), (42, 137, 126),\n","                 (155, 108, 249), (166, 148, 143), (81, 91, 87),\n","                 (100, 124, 51), (73, 131, 121), (157, 210, 220),\n","                 (134, 181, 60), (221, 223, 147), (123, 108, 131),\n","                 (161, 66, 179), (163, 221, 160), (31, 146, 98),\n","                 (99, 121, 30), (49, 89, 240), (116, 108, 9),\n","                 (161, 176, 169), (80, 29, 135), (177, 105, 197),\n","                 (139, 110, 246)]\n","\n","\n","class CrossEntropyLoss2d(nn.Module):\n","    def __init__(self, weight=med_frq):\n","        super(CrossEntropyLoss2d, self).__init__()\n","        self.ce_loss =F.mse_loss(out,depth)\n","\n","    def forward(self, inputs_scales, targets_scales):\n","        losses = []\n","        for inputs, targets in zip(inputs_scales, targets_scales):\n","            mask = targets > 0\n","            targets_m = targets.clone()\n","            targets_m[mask] -= 1\n","            loss_all = self.ce_loss(inputs, targets_m.long())\n","            losses.append(torch.sum(torch.masked_select(loss_all, mask)) / torch.sum(mask.float()))\n","        total_loss = sum(losses)\n","        return total_loss\n","\n","\n","def color_label(label):\n","    label = label.clone().cpu().data.numpy()\n","    colored_label = np.vectorize(lambda x: label_colours[int(x)])\n","\n","    colored = np.asarray(colored_label(label)).astype(np.float32)\n","    colored = colored.squeeze()\n","\n","    try:\n","        return torch.from_numpy(colored.transpose([1, 0, 2, 3]))\n","    except ValueError:\n","        return torch.from_numpy(colored[np.newaxis, ...])\n","\n","\n","def print_log(global_step, epoch, local_count, count_inter, dataset_size, loss, time_inter):\n","    print('Step: {:>5} Train Epoch: {:>3} [{:>4}/{:>4} ({:3.1f}%)]    '\n","          'Loss: {:.6f} [{:.2f}s every {:>4} data]'.format(\n","        global_step, epoch, local_count, dataset_size,\n","        100. * local_count / dataset_size, loss.data, time_inter, count_inter))\n","\n","\n","def save_ckpt(ckpt_dir, model, optimizer, global_step, epoch, local_count, num_train):\n","    # usually this happens only on the start of a epoch\n","    epoch_float = epoch + (local_count / num_train)\n","    state = {\n","        'global_step': global_step,\n","        'epoch': epoch_float,\n","        'state_dict': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","    }\n","    ckpt_model_filename = \"ckpt_epoch_{:0.2f}.pth\".format(epoch_float)\n","    path = os.path.join(ckpt_dir, ckpt_model_filename)\n","    torch.save(state, path)\n","    print('{:>2} has been successfully saved'.format(path))\n","\n","\n","def load_ckpt(model, optimizer, model_file, device):\n","    if os.path.isfile(model_file):\n","        print(model_file)\n","        print(\"=> loading checkpoint '{}'\".format(model_file))\n","        if device.type == 'cuda':\n","            checkpoint = torch.load(model_file)\n","        else:\n","            checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)\n","        model.load_state_dict(checkpoint['state_dict'])\n","        if optimizer:\n","            optimizer.load_state_dict(checkpoint['optimizer'])\n","        print(\"=> loaded checkpoint '{}' (epoch {})\"\n","              .format(model_file, checkpoint['epoch']))\n","        step = checkpoint['global_step']\n","        epoch = checkpoint['epoch']\n","        return step, epoch\n","    else:\n","        print(\"=> no checkpoint found at '{}'\".format(model_file))\n","        "],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JdkTaTybarol"},"source":["## **GENERATOR MODULE** => NO CHANGE"]},{"cell_type":"code","metadata":{"id":"RhT914YpZjnX","executionInfo":{"status":"ok","timestamp":1601639403047,"user_tz":-330,"elapsed":26460,"user":{"displayName":"smriti gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi61FdzStVeI4CbVBMrhSuqWIHqEw_fypYilNr4=s64","userId":"15960455078015597961"}}},"source":["#new model file\n","import torch\n","from torch import nn\n","import math\n","import torch.utils.model_zoo as model_zoo\n","from torch.utils.checkpoint import checkpoint\n","\n","\n","class RedNet(nn.Module):\n","    def __init__(self, pretrained=False):\n","#         print('it is in rednet init')\n","        super(RedNet, self).__init__()\n","        block = Bottleneck\n","        transblock = TransBasicBlock\n","        layers = [3, 4, 6, 3]\n","        # original resnet\n","        self.inplanes = 64\n","        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","\n","        # resnet for depth channel\n","        self.inplanes = 64\n","        self.conv1_d = nn.Conv2d(1,64, kernel_size=7, stride=2, padding=3,\n","                                 bias=False)\n","        self.bn1_d = nn.BatchNorm2d(64)\n","        self.layer1_d = self._make_layer(block,64, layers[0])\n","        self.layer2_d = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3_d = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer4_d = self._make_layer(block, 512, layers[3], stride=2)\n","\n","        self.inplanes = 512\n","        self.deconv1 = self._make_transpose(transblock, 256, 6, stride=2)\n","        self.deconv2 = self._make_transpose(transblock, 128, 4, stride=2)\n","        self.deconv3 = self._make_transpose(transblock, 64, 3, stride=2)\n","        self.deconv4 = self._make_transpose(transblock, 64, 3, stride=2)\n","\n","        self.agant0 = self._make_agant_layer(64, 64)\n","        self.agant1 = self._make_agant_layer(64 * 4, 64)\n","        self.agant2 = self._make_agant_layer(128 * 4, 128)\n","        self.agant3 = self._make_agant_layer(256 * 4, 256)\n","        self.agant4 = self._make_agant_layer(512* 4,512)\n","        # final block\n","        self.inplanes = 64\n","        self.final_conv = self._make_transpose(transblock, 64, 3)\n","\n","        self.final_deconv = nn.ConvTranspose2d(self.inplanes, 1,kernel_size=2,\n","                                               stride=2, padding=0, bias=True)\n","\n","#         self.out5_conv = nn.Conv2d(256,1, kernel_size=1, stride=1, bias=True)\n","#         self.out4_conv = nn.Conv2d(128,1, kernel_size=1, stride=1, bias=True)\n","#         self.out3_conv = nn.Conv2d(64,1, kernel_size=1, stride=1, bias=True)\n","#         self.out2_conv = nn.Conv2d(64, 1, kernel_size=1, stride=1, bias=True)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","                \n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","                \n","               \n","\n","        if pretrained:\n","            self._load_resnet_pretrained()\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","#         print('block exapnsion is',block.expansion)\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _make_transpose(self, block, planes, blocks, stride=1):\n","\n","        upsample = None\n","        if stride != 1:\n","            upsample = nn.Sequential(\n","                nn.ConvTranspose2d(self.inplanes, planes,\n","                                   kernel_size=2, stride=stride,\n","                                   padding=0, bias=False),\n","                nn.BatchNorm2d(planes),\n","            )\n","        elif self.inplanes != planes:\n","            upsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes),\n","            )\n","\n","        layers = []\n","\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, self.inplanes))\n","\n","        layers.append(block(self.inplanes, planes, stride, upsample))\n","        self.inplanes = planes\n","\n","        return nn.Sequential(*layers)\n","\n","    def _make_agant_layer(self, inplanes, planes):\n","\n","        layers = nn.Sequential(\n","            nn.Conv2d(inplanes, planes, kernel_size=1,\n","                      stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(planes),\n","            nn.ReLU(inplace=True)\n","        )\n","        return layers\n","\n","    def _load_resnet_pretrained(self):\n","        pretrain_dict = model_zoo.load_url(model_urls['resnet50'])\n","        model_dict = {}\n","        state_dict = self.state_dict()\n","        for k, v in pretrain_dict.items():\n","            if k in state_dict:\n","                if k.startswith('conv1'):  # the first conv_op\n","                    model_dict[k] = v\n","                    model_dict[k.replace('conv1', 'conv1_d')] = torch.mean(v, 1).data. \\\n","                        view_as(state_dict[k.replace('conv1', 'conv1_d')])\n","\n","                elif k.startswith('bn1'):\n","                    model_dict[k] = v\n","                    model_dict[k.replace('bn1', 'bn1_d')] = v\n","                elif k.startswith('layer'):\n","                    model_dict[k] = v\n","                    model_dict[k[:6] + '_d' + k[6:]] = v\n","        state_dict.update(model_dict)\n","        self.load_state_dict(state_dict)\n","\n","    def forward_downsample(self, rgb, depth):\n","\n","        x = self.conv1(rgb)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        depth = self.conv1_d(depth)\n","        depth = self.bn1_d(depth)\n","        depth = self.relu(depth)\n","\n","        fuse0 = x + depth\n","\n","        x = self.maxpool(fuse0)\n","        depth = self.maxpool(depth)\n","\n","        # block 1\n","        x = self.layer1(x)\n","        depth = self.layer1_d(depth)\n","        fuse1 = x + depth\n","        # block 2\n","        x = self.layer2(fuse1)\n","        depth = self.layer2_d(depth)\n","        fuse2 = x + depth\n","        # block 3\n","        x = self.layer3(fuse2)\n","        depth = self.layer3_d(depth)\n","        fuse3 = x + depth\n","        # block 4\n","        x = self.layer4(fuse3)\n","        depth = self.layer4_d(depth)\n","        fuse4 = x + depth\n","#         print('output of encoder',fuse4.size())\n","        return fuse0, fuse1, fuse2, fuse3, fuse4\n","\n","    def forward_upsample(self, fuse0, fuse1, fuse2, fuse3, fuse4):\n","\n","        agant4 = self.agant4(fuse4)\n","        # upsample 1\n","        x = self.deconv1(agant4)\n","#         if self.training:\n","#             out5 = self.out5_conv(x)\n","        x = x + self.agant3(fuse3)\n","        # upsample 2\n","        x = self.deconv2(x)\n","#         if self.training:\n","#             out4 = self.out4_conv(x)\n","        x = x + self.agant2(fuse2)\n","        # upsample 3\n","        x = self.deconv3(x)\n","#         if self.training:\n","#             out3 = self.out3_conv(x)\n","        x = x + self.agant1(fuse1)\n","        # upsample 4\n","        x = self.deconv4(x)\n","#         if self.training:\n","#             out2 = self.out2_conv(x)\n","        x = x + self.agant0(fuse0)\n","        # final\n","        x = self.final_conv(x)\n","        out = self.final_deconv(x)\n","\n","        if self.training:\n","            return out\n","        return out\n","\n","    def forward(self, rgb, depth, phase_checkpoint=False):\n","\n","        if phase_checkpoint:\n","            depth.requires_grad_()\n","            fuses = checkpoint(self.forward_downsample, rgb, depth)\n","            out = checkpoint(self.forward_upsample, *fuses)\n","        else:\n","            fuses = self.forward_downsample(rgb, depth)\n","            out = self.forward_upsample(*fuses)\n","\n","        return out\n","\n","\n","    def conv3x3(in_planes, out_planes, stride=1):\n","    #\"3x3 convolution with padding\"\n","        return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","class TransBasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, upsample=None, **kwargs):\n","        super(TransBasicBlock, self).__init__()\n","        self.conv1 = conv3x3(inplanes, inplanes)\n","        self.bn1 = nn.BatchNorm2d(inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        if upsample is not None and stride != 1:\n","            self.conv2 = nn.ConvTranspose2d(inplanes, planes,\n","                                            kernel_size=3, stride=stride, padding=1,\n","                                            output_padding=1, bias=False)\n","        else:\n","            self.conv2 = conv3x3(inplanes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.upsample = upsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.upsample is not None:\n","            residual = self.upsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"isgdKl_za31Y"},"source":["## **DISCRIMINATOR MODULE** => NO CHANGE"]},{"cell_type":"code","metadata":{"id":"mitZOVEba0Lm","executionInfo":{"status":"ok","timestamp":1601639403056,"user_tz":-330,"elapsed":25467,"user":{"displayName":"smriti gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi61FdzStVeI4CbVBMrhSuqWIHqEw_fypYilNr4=s64","userId":"15960455078015597961"}}},"source":["#new model file\n","import torch\n","from torch import nn\n","import math\n","import torch.utils.model_zoo as model_zoo\n","from torch.utils.checkpoint import checkpoint\n","\n","\n","class fus_disc(nn.Module):\n","    def __init__(self):\n","#         print('it is in rednet init')\n","        super(fus_disc, self).__init__()\n","        block = Bottleneck\n","        layers = [3, 4, 6, 3]\n","        # original resnet\n","        self.inplanes = 64\n","        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block,256, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","        self.final_conv=nn.Conv2d(512*4,128, stride=1, padding=0,kernel_size=1,bias=False)\n","        self.final_bn=nn.BatchNorm2d(128)\n","        self.flatten=Flatten()\n","        self.lin1=nn.Linear(128*15*20,2048)\n","        self.lin2=nn.Linear(2048,512)\n","        self.lin3=nn.Linear(512,512)\n","        self.lin4=nn.Linear(512,1)\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.normal_(m.weight.data, 1.0, 0.02)\n","                nn.init.constant_(m.bias.data, 0)\n","\n","#                 m.weight.data.fill_(1)\n","#                 m.bias.data.zero_()\n","#                 print (\"weight\",m.weight.shape)\n","            elif isinstance(m, nn.Linear):\n","                torch.nn.init.xavier_uniform(m.weight)\n","                m.bias.data.fill_(0.01)\n","                #m.bias.data.zero_()\n","        \n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","    def forward(self, rgb):\n","\n","        x = self.conv1(rgb)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        # block 1\n","        x = self.layer1(x)\n","        x=self.layer2(x)\n","        x=self.layer3(x)\n","        x=self.layer4(x)\n","        x=self.final_conv(x)\n","        x=self.final_bn(x)\n","        x=self.relu(x)\n","        x=self.flatten(x)\n","        x = F.dropout(self.lin1(x), p=0.2, training=self.training)\n","        x = F.relu(x)\n","        x = F.dropout(self.lin2(x), p=0.2, training=self.training)\n","        x = F.relu(x)\n","        x = F.dropout(self.lin3(x), p=0.2, training=self.training)\n","        x = F.relu(x)\n","        x = F.dropout(self.lin4(x), p=0.2, training=self.training)\n","        x = F.sigmoid(x)\n","        return x\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    \"3x3 convolution with padding\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","class Flatten(nn.Module):\n","    def forward(self, input):\n","        return input.view(input.size(0), -1)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIrH_aBta-BL"},"source":["## **ARGUMENTS** "]},{"cell_type":"code","metadata":{"id":"i2WOKDyDbDNn","executionInfo":{"status":"ok","timestamp":1601639403058,"user_tz":-330,"elapsed":23877,"user":{"displayName":"smriti gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi61FdzStVeI4CbVBMrhSuqWIHqEw_fypYilNr4=s64","userId":"15960455078015597961"}}},"source":["args = easydict.EasyDict({\n","        \"description\": 'RedNet fusion',\n","        \"cuda\":True,\n","        \"workers\": 0,\n","        \"epochs\": 10,\n","        \"start_epoch\": 0,\n","        \"batch_size\":1,\n","        \"lr_gen\":0.0002, #geberator learning rate\n","        \"lr_disc\":0.0001, #discriminator learning rate\n","        \"print_freq\":200,\n","        \"save_epoch_freq\":5,\n","        \"last_ckpt\":False, # if further training a pretrained model\n","        \"last_ckp_location_generator\":\"\", #give location of last checkpoint (only for generator) \n","        \"lr_decay_rate\":0.8,\n","        \"lr_epoch_per_decay\":100,\n","        \"ckpt_dir_model\":'./model',\n","        \"ckpt_dir_disc\":'./disc',\n","        \"checkpoint\":False,\n","        \"beta\":0.5,\n","        \"image_w\":640,\n","        \"image_h\":480,\n","        \"EPS\" :1e-15, #epsilon for adversarial attack\n","        \"REGULARIZATION\" : 1e-6,\n","        \"eps\":1e-12 #for normalisation\n","\n","\n","})"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bp2Z6rbpbNNH"},"source":["## **TRAINING INITIALIZATION**"]},{"cell_type":"code","metadata":{"id":"lP5ju_j0bNa7","executionInfo":{"status":"ok","timestamp":1601639406625,"user_tz":-330,"elapsed":26217,"user":{"displayName":"smriti gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi61FdzStVeI4CbVBMrhSuqWIHqEw_fypYilNr4=s64","userId":"15960455078015597961"}},"outputId":"2b942cc4-54be-421f-bf07-8d18669522b4","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","global_step = 0\n","\n","device = torch.device(\"cuda:0\" if args.cuda and torch.cuda.is_available() else \"cpu\")\n","\n","image_w = args.image_w\n","image_h = args.image_h\n","train_data = SUNRGBD(transform=transforms.Compose([ToTensor()]),phase_train=True)\n","train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True,num_workers=args.workers, pin_memory=False)\n","num_train = len(train_loader)\n","criterion=torch.nn.BCELoss()\n","criterion.to(device)\n","# print(num_train)\n","# print(\"hit_1\")\n","torch.cuda.manual_seed_all(47)\n","\n","#initializing models\n","model = RedNet(pretrained=False)\n","if args.last_ckpt:\n","  load_ckpt(model, None, args.last_ckp_location_generator, device)\n","disc=fus_disc()\n","\n","#parallelising\n","if torch.cuda.device_count() > 1:\n","    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n","    model = nn.DataParallel(model)\n","    disc = nn.DataParallel(disc)\n","\n","#initializing optimizers\n","optim_ae = torch.optim.Adam(model.parameters(), lr=0.0002) #generator optimizer\n","optim_D = torch.optim.Adam(disc.parameters(), lr=0.0001) #discriminator optimizer\n","\n","lr_decay_lambda = lambda epoch: args.lr_decay_rate ** (epoch // args.lr_epoch_per_decay)\n","scheduler = LambdaLR(optim_ae, lr_lambda=lr_decay_lambda)\n","lr_decay_disc = lambda epoch: args.lr_decay_rate \n","schedulerD = LambdaLR(optim_D, lr_lambda=lr_decay_disc)\n","\n","lossmse = []\n","losstv = []\n","lossae= []\n","lossG = []\n","lossd = []\n","lossspec = []\n","model.train()\n","disc.train()\n","disc.train()\n","model.to(device)\n","disc.to(device)\n","eps=args.eps\n","EPS=args.EPS"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"e_JZWJqUczaH"},"source":["## ***TRAINING***"]},{"cell_type":"code","metadata":{"id":"Y_9y9_bHczma","outputId":"2e04ea47-5aa2-4de2-cd74-7ea80da501e4","colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["\n","for epoch in range(int(args.start_epoch), args.epochs):\n","    schedulerD.step(epoch)\n","    local_count = 0\n","    last_count = 0\n","    end_time = time.time()\n","    if epoch % args.save_epoch_freq == 0 and epoch != args.start_epoch:\n","        save_ckpt(args.ckpt_dir_model, model, optim_ae, global_step, epoch,\n","                     local_count, num_train)\n","        save_ckpt(args.ckpt_dir_disc, disc, optim_D, global_step, epoch,\n","                     local_count, num_train)\n","    for batch_idx, sample in enumerate(train_loader):\n","        optim_ae.zero_grad()      \n","        optim_ae.step()  \n","        image = sample['image'].to(device)\n","        depth = sample['depth'].to(device)  \n","        output = model(depth, image, args.checkpoint)\n","        \n","        #formulation of losses\n","        \n","        loss1 =(args.beta*F.mse_loss(output, image)+(1-args.beta)*F.mse_loss(output, depth))\n","        \n","        output_trans = output.view((-1,1))\n","        depth_trans = depth.view((-1,1))\n","        z = torch.sum(torch.mul(output_trans,depth_trans))\n","        \n","        tv_out = args.REGULARIZATION * (\n","                    torch.sum(torch.abs(output[:, :, :, :-1] - output[:, :, :, 1:])) + \n","                    torch.sum(torch.abs(output[:, :, :-1, :] - output[:, :, 1:, :])))        \n","        tv_image = args.REGULARIZATION * (\n","                    torch.sum(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:])) + \n","                    torch.sum(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :])))        \n","        loss2 = F.mse_loss(tv_image,tv_out)\n","        #normalize\n","        norm_out = torch.norm(output_trans)\n","        norm_depth = torch.norm(depth_trans)        \n","        prod =  torch.div(z,norm_out*norm_depth)\n","        loss_3 = torch.acos(prod)\n","\n","        G_loss = torch.mean(torch.log(1-disc_fake + EPS))\n","        \n","        #FINAL GENERATOR LOSS\n","        loss = 1.1*(loss1+loss2)+G_loss+ 0.002*(loss_3)\n","        #LOSS BACK PROPOGATION\n","        loss.backward(retain_graph=True)\n","        optim_ae.step()\n","        \n","        #DISCRIMINATOR LOSS\n","        optim_D.zero_grad()\n","        disc_real=disc(image)\n","        out_normalised = torch.nn.functional.normalize(output, p=2, dim=1,eps=eps, out=None)\n","        disc_fake=disc(out_normalised)\n","        disc_loss = -torch.mean(torch.log(disc_real + EPS) + torch.log(1 - disc_fake + EPS))\n","        disc_loss.backward(retain_graph=True)\n","        optim_D.step()\n","        \n","        lossspec.append(loss_3.item().data)\n","\n","        #model.eval()\n","        \n","        #STORE LOSSES\n","        lossmse.append(loss1.item().data)\n","        lossG.append(G_loss.item().detach())\n","        losstv.append(loss2.item().detach())\n","        lossae.append(loss.item().detach())  \n","        lossd.append(disc_loss.item().detach())\n","        lossspec.append(loss_3.item().detach())\n","        \n","        local_count += image.data.shape[0]\n","        global_step += 1\n","\n","        if global_step % args.print_freq == 0 or global_step == 1:           \n","            time_inter = time.time() - end_time\n","            count_inter = local_count - last_count\n","            print_log(global_step, epoch, local_count, count_inter,\n","                          num_train, loss, time_inter)\n","            print_log(global_step, epoch, local_count, count_inter,\n","                          num_train, disc_loss, time_inter)\n","            end_time = time.time()\n","\n","save_ckpt(args.ckpt_dir_model, model, optim_ae, global_step, args.epochs,\n","              0, num_train)\n","save_ckpt(args.ckpt_dir_disc, disc, optim_D, global_step, args.epochs,\n","              0, num_train)\n","print(\"Training completed \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"V6dbAVHJ19eJ"},"source":[""],"execution_count":null,"outputs":[]}]}